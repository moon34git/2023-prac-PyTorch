{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jhmoon/nltk_data',\n",
       " '/home/jhmoon/venvTORCH/env/nltk_data',\n",
       " '/home/jhmoon/venvTORCH/env/share/nltk_data',\n",
       " '/home/jhmoon/venvTORCH/env/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jhmoon/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"Is it possible distinguishing cats and dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('possible', 'JJ'),\n",
       " ('distinguishing', 'VBG'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'favorite', 'subject', 'is', 'math']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = \"my favorite subject is math\"\n",
    "string2 = \"my favorite subject is math, english, economic and computer science\"\n",
    "nltk.word_tokenize(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'subject',\n",
       " 'is',\n",
       " 'math',\n",
       " ',',\n",
       " 'english',\n",
       " ',',\n",
       " 'economic',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'science']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language.', 'In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of         human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language. \\\n",
    "    In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of \\\n",
    "        human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.'\n",
    "\n",
    "tokenized_sentences = sent_tokenize(text_sample)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'book', 'is', 'for', 'deep', 'learning', 'learners']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"This book is for deep learning learners\"\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nothing',\n",
       " 'that',\n",
       " 'you',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'already',\n",
       " 'know',\n",
       " 'except',\n",
       " 'most',\n",
       " 'people',\n",
       " 'aren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'how',\n",
       " 'their',\n",
       " 'inner',\n",
       " 'world',\n",
       " 'works',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "sentence = \"it's nothing that you don't already know except most people aren't aware of how their inner world works.\"\n",
    "words = WordPunctTokenizer().tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'things',\n",
       " 'that',\n",
       " 'we',\n",
       " 'ask',\n",
       " 'ourselves',\n",
       " 'is',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'pros',\n",
       " 'and',\n",
       " 'cons',\n",
       " 'of',\n",
       " 'any',\n",
       " 'task',\n",
       " 'we',\n",
       " 'perform',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sample_text = \"One of the first things that we ask ourselves is what are the pros and cons of any task we perform.\"\n",
    "text_tokens = word_tokenize(sample_text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One', 'first', 'things', 'ask', 'pros', 'cons', 'task', 'perform', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'standardize'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma.lemmatize('standardizes', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/diabetes.csv\")\n",
    "X = df[df.columns[:-1]]\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = torch.tensor(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y_train = ms.fit_transform(y_train)\n",
    "y_test = ms.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16444009, -0.56227168, -0.24430085, ...,  1.11957077,\n",
       "         0.03679193, -1.02896448],\n",
       "       [-1.16444009, -1.1107311 , -0.24430085, ...,  0.50619724,\n",
       "         0.18390123, -1.02896448],\n",
       "       [-0.24833467, -0.40992184, -0.34647387, ..., -1.14224411,\n",
       "         0.58319791, -0.86360955],\n",
       "       ...,\n",
       "       [ 0.36240227, -0.13569213,  0.26656423, ...,  0.05894571,\n",
       "         0.52915776,  0.12852003],\n",
       "       [ 1.58387616,  1.50968613,  0.26656423, ...,  1.55404369,\n",
       "        -0.24241552,  0.78993975],\n",
       "       [ 2.80535005,  0.99169668,  0.98177534, ...,  1.11957077,\n",
       "         2.07230431,  0.45922989]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1644, -0.5623, -0.2443,  ...,  1.1196,  0.0368, -1.0290],\n",
       "        [-1.1644, -1.1107, -0.2443,  ...,  0.5062,  0.1839, -1.0290],\n",
       "        [-0.2483, -0.4099, -0.3465,  ..., -1.1422,  0.5832, -0.8636],\n",
       "        ...,\n",
       "        [ 0.3624, -0.1357,  0.2666,  ...,  0.0589,  0.5292,  0.1285],\n",
       "        [ 1.5839,  1.5097,  0.2666,  ...,  1.5540, -0.2424,  0.7899],\n",
       "        [ 2.8054,  0.9917,  0.9818,  ...,  1.1196,  2.0723,  0.4592]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = customDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "test_data = customDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 64, bias = True)\n",
    "        self.layer2 = nn.Linear(64, 64, bias = True)\n",
    "        self.layer3 = nn.Linear(64, 1, bias = True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 1001\n",
    "print_epoch = 100\n",
    "lr = 1e-2\n",
    "\n",
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "BCE = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    # print(y_pred_tag)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    # print(correct_results_sum)\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch: 0 - loss: 0.41724; acc: 82.111\n",
      "Test: epoch: 0 - loss: 0.41278; acc: 80.000\n",
      "Train: epoch: 100 - loss: 0.42368; acc: 77.556\n",
      "Test: epoch: 100 - loss: 0.41853; acc: 80.750\n",
      "Train: epoch: 200 - loss: 0.49502; acc: 77.889\n",
      "Test: epoch: 200 - loss: 0.41190; acc: 81.500\n",
      "Train: epoch: 300 - loss: 0.48342; acc: 77.556\n",
      "Test: epoch: 300 - loss: 0.38376; acc: 81.500\n",
      "Train: epoch: 400 - loss: 0.39755; acc: 84.778\n",
      "Test: epoch: 400 - loss: 0.38068; acc: 80.750\n",
      "Train: epoch: 500 - loss: 0.39887; acc: 79.667\n",
      "Test: epoch: 500 - loss: 0.39038; acc: 82.500\n",
      "Train: epoch: 600 - loss: 0.41103; acc: 79.111\n",
      "Test: epoch: 600 - loss: 0.37980; acc: 82.500\n",
      "Train: epoch: 700 - loss: 0.41751; acc: 81.333\n",
      "Test: epoch: 700 - loss: 0.44163; acc: 79.250\n",
      "Train: epoch: 800 - loss: 0.41535; acc: 77.111\n",
      "Test: epoch: 800 - loss: 0.40436; acc: 81.250\n",
      "Train: epoch: 900 - loss: 0.41695; acc: 76.222\n",
      "Test: epoch: 900 - loss: 0.37890; acc: 82.250\n",
      "Train: epoch: 1000 - loss: 0.37916; acc: 79.000\n",
      "Test: epoch: 1000 - loss: 0.37102; acc: 85.250\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    iteration_loss = 0.\n",
    "    iteration_accuracy = 0.\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss = BCE(y_pred, y.reshape(-1, 1).float())\n",
    "        \n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % print_epoch == 0:\n",
    "        print(f'Train: epoch: {epoch} - loss: {(iteration_loss/(i+1)):.5f}; acc: {(iteration_accuracy/(i+1)):.3f}')\n",
    "    \n",
    "    iteration_loss = 0.\n",
    "    iteration_accuracy = 0.\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss = BCE(y_pred, y.reshape(-1, 1).float())\n",
    "        \n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % print_epoch == 0:\n",
    "        print(f'Test: epoch: {epoch} - loss: {(iteration_loss/(i+1)):.5f}; acc: {(iteration_accuracy/(i+1)):.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "# print(y_pred_tag)\n",
    "correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "# print(correct_results_sum)\n",
    "acc = correct_results_sum / y_test.shape[0]\n",
    "acc = torch.round(acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0570,  1.6011,  0.0622, -0.4102,  0.7528, -0.2733, -0.3685, -0.0368],\n",
       "        [-1.1644,  0.2299, -0.0400, -0.0941,  0.8533, -0.1711,  2.7238, -0.6983]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7397],\n",
       "        [-0.6675]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], device='cuda:0', grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tag = torch.round(torch.sigmoid(y_pred)); y_pred_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_results_sum = (y_pred_tag == y_test); correct_results_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
